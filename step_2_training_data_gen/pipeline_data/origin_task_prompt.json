{
    "0": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "1": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "2": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "3": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "4": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "5": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "6": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "7": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "8": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_checklist_item\nDescription: a button; used to add a new checklist item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "9": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: move_selected_item_to_the_top\nDescription: a button \"Move to the top\"; used to move the selected item to the top of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: move_selected_item_to_the_buttom\nDescription: a button \"Move to the bottom\"; used to move the selected checklist item to the bottom of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "10": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: move_selected_item_to_the_buttom\nDescription: a button \"Move to the bottom\"; used to move the selected checklist item to the bottom of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "11": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "12": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "13": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "14": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "15": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "16": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "17": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "18": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "19": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "20": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "21": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "22": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "23": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "24": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "25": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "26": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "27": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "28": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "29": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "30": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "31": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "32": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "33": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "34": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "35": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "36": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "37": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "38": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "39": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "40": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "41": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "42": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "43": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "44": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "45": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "46": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "47": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "48": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "49": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "50": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "51": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "52": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "53": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "54": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "55": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "56": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "57": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "58": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "59": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "60": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "61": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "62": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "63": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "64": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "65": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "66": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "67": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "68": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "69": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "70": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "71": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "72": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "73": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "74": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "75": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "76": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "77": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "78": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "79": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "80": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "81": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "82": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "83": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "84": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "85": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "86": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "87": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "88": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "89": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "90": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "91": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "92": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: import_notes\nDescription: a button; used to import notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "93": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: import_notes\nDescription: a button; used to import notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_enable_automatic_backups\nDescription: a checkbox; used to set enable automatic backups\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "94": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "95": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "96": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "97": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "98": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "99": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "100": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: add_checklist_item\nDescription: a button; used to add a new checklist item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "101": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: move_selected_item_to_the_top\nDescription: a button \"Move to the top\"; used to move the selected item to the top of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: move_selected_item_to_the_buttom\nDescription: a button \"Move to the bottom\"; used to move the selected checklist item to the bottom of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "102": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: move_selected_item_to_the_buttom\nDescription: a button \"Move to the bottom\"; used to move the selected checklist item to the bottom of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "103": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "104": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "105": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "106": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "107": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "108": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "109": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "110": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "111": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "112": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "113": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "114": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "115": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "116": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "117": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "118": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "119": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "120": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "121": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "122": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "123": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "124": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "125": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "126": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "127": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "128": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "129": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "130": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "131": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "132": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "133": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "134": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "135": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "136": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "137": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "138": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "139": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "140": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "141": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "142": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "143": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "144": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "145": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "146": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "147": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "148": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "149": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "150": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "151": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "152": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "153": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "154": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "155": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "156": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "157": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "158": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "159": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "160": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "161": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "162": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "163": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "164": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "165": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "166": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "167": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "168": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "169": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "170": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "171": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "172": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "173": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "174": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "175": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "176": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "177": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "178": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "179": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "180": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "181": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "182": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: import_notes\nDescription: a button; used to import notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "183": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: import_notes\nDescription: a button; used to import notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_automatic_backups\nDescription: a checkbox; used to set enable automatic backups\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "184": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "185": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "186": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "187": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "188": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: add_checklist_item\nDescription: a button; used to add a new checklist item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "189": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: move_selected_item_to_the_top\nDescription: a button \"Move to the top\"; used to move the selected item to the top of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: move_selected_item_to_the_buttom\nDescription: a button \"Move to the bottom\"; used to move the selected checklist item to the bottom of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "190": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: move_selected_item_to_the_buttom\nDescription: a button \"Move to the bottom\"; used to move the selected checklist item to the bottom of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "191": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "192": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "193": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "194": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "195": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "196": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "197": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "198": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "199": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "200": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "201": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "202": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "203": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "204": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "205": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "206": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "207": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "208": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "209": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "210": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "211": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "212": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "213": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "214": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "215": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "216": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "217": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "218": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "219": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "220": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "221": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "222": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "223": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "224": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "225": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "226": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "227": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "228": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "229": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "230": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "231": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "232": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "233": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "234": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "235": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "236": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "237": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "238": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "239": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "240": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "241": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "242": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "243": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "244": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "245": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "246": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "247": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "248": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "249": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "250": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "251": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "252": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "253": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "254": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "255": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "256": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "257": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "258": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "259": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "260": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "261": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "262": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "263": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "264": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "265": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "266": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "267": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "268": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: import_notes\nDescription: a button; used to import notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "269": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: import_notes\nDescription: a button; used to import notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_automatic_backups\nDescription: a checkbox; used to set enable automatic backups\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "270": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "271": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "272": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_item\nDescription: a scrollbar; used to switch previous or next item which is a checklist or a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_prev_item\nDescription: a button; used to switch the previous item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: item_title\nDescription: a text showed current item title; used to show the title of the current item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: switch_to_next_item\nDescription: a button; used to switch the next item of checklist or note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: add_checklist_item\nDescription: a button; used to add a new checklist item\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "273": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: move_selected_item_to_the_top\nDescription: a button \"Move to the top\"; used to move the selected item to the top of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: move_selected_item_to_the_buttom\nDescription: a button \"Move to the bottom\"; used to move the selected checklist item to the bottom of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "274": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: move_selected_item_to_the_buttom\nDescription: a button \"Move to the bottom\"; used to move the selected checklist item to the bottom of the list\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "275": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_selected_checklist_item\nDescription: a button \"Rename\"; used to rename the selected checklist item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "276": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_selected_checklist_item\nDescription: a button \"Delete\"; used to delete the selected item\nDependency: this UI element could be interacted after touch(checklist_item)\n\nelement: checklist_item_list\nDescription: a element list of checklist items; used to show and match all checklist items\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_item\nDescription: a button; used to show the content and the state of the checklist item\nDependency: this UI element could be interacted after match(checklist_item_list)\n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "277": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note_title_list\nDescription: A list of open_note_title elements; used to display the titles of notes to open\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "278": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_note_title\nDescription: a button; used to open the note with the title\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "279": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: save_note\nDescription: a button; used to save the current note after changed text_note_content\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "280": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "281": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "282": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: note_content\nDescription: a input; used to show a large editable text area; used to edit the content of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "283": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: search_text\nDescription: a input; used to input the text to search within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "284": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: prev_matched_text\nDescription: a button \"Previous\"; used to search the previous matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "285": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: next_matchhed_text\nDescription: a button \"Next\"; used to search the next matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "286": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: close_search_box\nDescription: a button; used to close the search box of matching text within the current note\nDependency: this UI element could be interacted after touch(search_within_note)\n\nelement: search_within_note\nDescription: a button \"Search\"; used to search within the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "287": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_title\nDescription: a input \"label\"; used to input the title of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "288": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_types\nDescription: a element tuple including text_note_type and checklist_type; used to show the types of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "289": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: text_note_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "290": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: checklist_type\nDescription: a checkbox; used to select the type of the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "291": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: add_note_ok\nDescription: a button \"OK\"; used to confirm the new note\nDependency: this UI element could be interacted after touch(create_note) or touch(create_note)\n\nelement: create_note\nDescription: A button ; used to add a new note; tap to open a window containing add_note_title, add_note_types, add_note_ok\nDependency: this UI element could be interacted after touch(open_note)\n\nelement: open_note\nDescription: A button \"Open Note\"; used to open a note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "292": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "293": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: create_shortcut\nDescription: a button \"Create shortcut\"; used to create a shortcut of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "294": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "295": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: open_file\nDescription: a button \"Open file\"; used to open the file of the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "296": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: export_as_file\nDescription: a button \"Export as file\"; used to export the current note as a file\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "297": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: print_note\nDescription: a button \"Print\"; used to print the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "298": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "299": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "300": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: about\nDescription: a button \"About\"; used to show the information of the app\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "301": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_input\nDescription: a input; used to input the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "302": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: rename_ok\nDescription: a button \"OK\"; used to confirm the new name of the current note\nDependency: this UI element could be interacted after touch(rename_note) or touch(rename_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: rename_note\nDescription: a button \"Rename note\"; used to rename the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "303": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: lock_note_ok\nDescription: a button \"OK\"; used to confirm the lock of the current note\nDependency: this UI element could be interacted after touch(lock_note) or touch(lock_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: lock_note\nDescription: a button \"Lock note\"; used to lock the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "304": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: delete_note_ok\nDescription: a button \"Delete\"; used to confirm the delete of the current note\nDependency: this UI element could be interacted after touch(delete_note) or touch(delete_note)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: delete_note\nDescription: a button \"Delete note\"; used to delete the current note\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "305": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: remove_done_checklist_items\nDescription: a button \"Remove done items\"; used to remove the done items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "306": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "307": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items_types\nDescription: a element tuple including sort_by_title, sort_by_date_created, sort_by_custom; used to show the types of sorting the items of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "308": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: title_type\nDescription: a checkbox \"Title\"; used to sort the items of the current checklist by title\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "309": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: date_created_type\nDescription: a checkbox \"Date created\"; used to sort the items of the current checklist by Date created\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "310": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: custom_type\nDescription: a checkbox \"Custom\"; used to sort the items of the current checklist by Custom\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "311": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: sort_checklist_items_ok\nDescription: a button \"OK\"; used to confirm the sort type of the current checklist\nDependency: this UI element could be interacted after touch(sort_checklist_items)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: sort_checklist_items\nDescription: a button \"Sort by\"; used to sort the items of the current checklist\nDependency: this UI element could be interacted after touch(more_options_checklist)\n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "312": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: exit_settings\nDescription: a button \"Back\"; used to exit the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "313": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: scroll_settings_page\nDescription: a scrollbar; used to scroll the settings page\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "314": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "315": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_widget_colors\nDescription: a button \"Customize widget colors\"; used to customize the colors of the widget\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "316": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "317": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "318": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_show_word_count\nDescription: a checkbox \"Show word count\"; used to show the word count of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "319": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_make_links_and_emails_clickable\nDescription: a checkbox \"Make links and emails clickable\"; used to make links and emails clickable\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "320": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_use_monospaced_font\nDescription: a checkbox \"use monospaced font\"; used to use monospaced font\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "321": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_use_incognito_mode_of_keyboards\nDescription: a checkbox \"Use Incognito mode of keyboards\"; used to use Incognito mode of keyboards\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "322": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_enable_line_wrap\nDescription: a checkbox \"Enable line wrap\"; used to enable line wrap\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "323": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_alignment\nDescription: a button \"Alignment\"; used to set the alignment of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "324": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: close_set_colors\nDescription: a button \"Close\"; used to close the popup window of setting colors\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "325": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: save_set_colors\nDescription: a button \"Save\"; used to save the colors of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "326": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "327": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_text_color\nDescription: a button \"Text color\"; used to set the text color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "328": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_background_color\nDescription: a button \"Background color\"; used to set the background color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "329": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_primary_color\nDescription: a button \"Primary color\"; used to set the primary color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "330": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_app_icon_color\nDescription: a button \"App icon color\"; used to set the app icon color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "331": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_types\nDescription: a element tuple including auto_light_drak, light, dark, dark_red, white, black_white, custom; used to show the theme color types of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "332": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_auto_light_dark\nDescription: a checkbox \"Auto light / dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "333": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_light\nDescription: a checkbox \"light\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "334": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_dark\nDescription: a checkbox \"Dark\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "335": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_dark_red\nDescription: a checkbox \"Dark red\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "336": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_white\nDescription: a checkbox \"White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "337": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_black_white\nDescription: a checkbox \"Black & White\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "338": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: theme_color_custom\nDescription: a checkbox \"Custom\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_theme_color) or touch(set_theme_color)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_colors\nDescription: a button \"Customize colors\"; used to customize the colors of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_theme_color\nDescription: a button \"Theme\"; used to set the theme color of the app\nDependency: this UI element could be interacted after touch(set_colors) or touch(set_colors)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "339": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_types\nDescription: a element tuple including 50%, 60%, 75%, 90%, 100%, 125%, 150%, 175%, 200%, 250%, 300%; used to show the font size types of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "340": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_50%\nDescription: a checkbox \"50%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "341": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_60%\nDescription: a checkbox \"60%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "342": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_75%\nDescription: a checkbox \"75%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "343": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_90%\nDescription: a checkbox \"90%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "344": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_100%\nDescription: a checkbox \"100%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "345": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_125%\nDescription: a checkbox \"125%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "346": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_150%\nDescription: a checkbox \"150%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "347": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_175%\nDescription: a checkbox \"175%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "348": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_200%\nDescription: a checkbox \"200%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "349": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_250%\nDescription: a checkbox \"250%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "350": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: font_size_300%\nDescription: a checkbox \"300%\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(set_font_size) or touch(current_font_size) or touch(set_font_size) or touch(current_font_size)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: set_font_size\nDescription: a button \"Font size\"; used to set the font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: current_font_size\nDescription: a button; used to show the current font size of the app\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: import_notes\nDescription: a button; used to import notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**",
    "351": "Suppose you are a dataset annotator who is working on generating a series of tasks about the Notes APP on a smartphone. You are given a series of UI elements in this APP, which you could interact with by tapping, long tapping, edit, scroll, etc. You should generate as many specific tasks that could be executed by a virtual assistant on a smartphone as possible. Note that the tasks you generate must only involve these elements. \n\nUI elements in the Notes APP: \n\nelement: set_place_cursor_to_the_end_of_note\nDescription: a checkbox; used to set the place cursor to the end of note\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: more_options_note\nDescription: A button \"more options\"; used to open more options of the current note\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: settings\nDescription: a button \"Settings\"; used to open the settings of the app, including #todo\nDependency: this UI element could be interacted after touch(more_options_note) or touch(more_options_checklist)\n\nelement: more_options_checklist\nDescription: A button \"more options\"; used to open more options of the current checklist\nDependency: No dependency, this UI element is in the main screen of the app. \n\nelement: set_show_keyboard_on_startup\nDescription: a checkbox; used to set the show keyboard on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_show_a_note_picker_on_startup\nDescription: a checkbox; used to set show a note picker on startup\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_autosave_notes\nDescription: a checkbox; used to set autosave notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_display_save_success_messages\nDescription: a checkbox; used to set display save success messages\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: export_notes\nDescription: a button; used to export notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: import_notes\nDescription: a button; used to import notes\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nelement: set_enable_automatic_backups\nDescription: a checkbox; used to set enable automatic backups\nDependency: this UI element could be interacted after touch(settings) or touch(settings)\n\nPlease write down the tasks you would like to generate. You must use the following JSON format:\n\n[\"<Task 1>\", \"<Task 2>\", \"<Task 3>\"...]\n\nNow please generate the specifc tasks. Notice that:\n- You should be specific and avoid vague tasks. for example, you are forbidden to give tasks like \"Send a message\", instead, you should say \"Send a message 'good morning' to Alice\". Namely, you should be ensure your task descriptions are detailed, incorporating elements like names, accounts, phone numbers, and addresses.\n- Focus on end-user actions rather than detailing every step or UI element involved. Your tasks should mimic natural language commands that a user might give to a virtual assistant like Siri. \n- **Please do not output anything else but the JSON content**"
}